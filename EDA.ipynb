{
 "nbformat": 4,
 "nbformat_minor": 2,
 "metadata": {
  "language_info": {
   "name": "python",
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "version": "3.7.6-final"
  },
  "orig_nbformat": 2,
  "file_extension": ".py",
  "mimetype": "text/x-python",
  "name": "python",
  "npconvert_exporter": "python",
  "pygments_lexer": "ipython3",
  "version": 3,
  "kernelspec": {
   "name": "python37664bitdsproj10320conda7d2d0b8284c34340aa156a3daf2ddb47",
   "display_name": "Python 3.7.6 64-bit ('DS_proj1_03-20': conda)"
  }
 },
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Complete Notebook\n",
    "## Module Loader"
   ],
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd \n",
    "import numpy as np \n",
    "import matplotlib.pyplot as plt \n",
    "import seaborn as sns\n",
    "from scipy import stats\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "sns.set()\n",
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Analysis (General)\n",
    "### Dataloader"
   ],
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "path = r\"data\\train.csv\"\n",
    "X_raw = pd.read_csv(path, index_col=\"Id\")\n",
    "X_ana = X_raw.copy()\n",
    "X_test = pd.read_csv(path, index_col =\"Id\")\n",
    "print(X_raw.columns)\n",
    ""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### General analysis\n",
    "\n",
    "**The Problem:**  \n",
    "We want to predict houseprices based on several parameters of different segments.\n",
    "\n",
    "Steps:  \n",
    "A. Read the docs and get information about the dataset, its gathering and the environment of the dataset  \n",
    "B. Analyse column by columns and make information about Type, Segment, Expected Importance  \n",
    "C. Test the assumptions with box / scatter plots  \n",
    "D. Write down consolidated information about importance  \n",
    "\n"
   ],
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "A. Dataset information\n",
    "Unfortunately no additional information were provided within the competition  \n",
    "  \n",
    "B. Analysis of every column\n",
    "After revising every column and its describtion from the data_description.csv-file I have segmented and prioritized the different variabels.  \n",
    "The most important ones form my perspective are:  \n",
    "* LotArea\n",
    "* Utilities\n",
    "* Neighborhood\n",
    "* BldgType\n",
    "* OverallQual\n",
    "* OverallCond\n",
    "* TotalBsmtSF\n",
    "* 1stFlrSF\n",
    "* GrLivArea\n",
    "* FullBath\n",
    "* BedroomAbvGr\n",
    "* TotRmsAbvGrd\n",
    "* GarageArea\n",
    "\n",
    "Next I will try to verify this assumptions with a correlation analysis"
   ],
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#X_corr = X_ana.dropna(axis=1)\n",
    "corrmat = X_ana.corr() #X_corr.corr()\n",
    "fig, ax = plt.subplots(figsize=(12, 9))\n",
    "ax1 = sns.heatmap(corrmat)\n",
    "plt.show()\n",
    "fig2, _ =plt.subplots(figsize=(12,9))\n",
    "ax2 = sns.heatmap(corrmat.iloc[:, -1:].nlargest(100, corrmat.columns[-1]))\n",
    "fig3, _ = plt.subplots(figsize=(25,18))\n",
    "\n",
    "n_all = len(X_ana.columns)\n",
    "n_specific = 16\n",
    "\n",
    "cols = corrmat.nlargest(n_specific, \"SalePrice\")[\"SalePrice\"].index \n",
    "cols_all = corrmat.nlargest(n_all, \"SalePrice\")[\"SalePrice\"].index\n",
    "#found missings in X_ana -> resulting in failure of corrcoef\n",
    "#print(X_ana[cols].isna().any().sum())\n",
    "#remove rows with missing values\n",
    "X_ana_no_nan = X_ana.dropna(subset=cols_all, axis=0)\n",
    "\n",
    "#use cleaned DF to proceed here\n",
    "cm = np.corrcoef(X_ana_no_nan[cols_all].values, rowvar=False)\n",
    "ax3 = sns.heatmap(cm, cbar=True,annot=True, xticklabels=cols.values, yticklabels=cols.values)\n",
    "\n",
    "print(cols.values[1:16], cols.values[-16: -1])\n",
    ""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The Top 15 most correlated columns according to this correlation analysis are  \n",
    "'OverallQual'  \n",
    "'GrLivArea'  \n",
    "'GarageCars'  \n",
    "'GarageArea'  \n",
    "'TotalBsmtSF'  \n",
    "'1stFlrSF'  \n",
    "'FullBath'  \n",
    "'TotRmsAbvGrd'  \n",
    "'YearBuilt'  \n",
    "'YearRemodAdd'  \n",
    "'GarageYrBlt'  \n",
    "'MasVnrArea'  \n",
    "'Fireplaces'  \n",
    "'BsmtFinSF1'  \n",
    "'LotFrontage'   \n",
    "\n",
    "The 15 least correlated columns according to this correlation analysis are  \n",
    "'BsmtUnfSF'  \n",
    "'BedroomAbvGr'  \n",
    "'ScreenPorch'  \n",
    "'PoolArea'  \n",
    "'MoSold'  \n",
    "'3SsnPorch'   \n",
    "'BsmtFinSF2'  \n",
    "'BsmtHalfBath'  \n",
    "'MiscVal'  \n",
    "'LowQualFinSF'  \n",
    "'YrSold'    \n",
    "'OverallCond'   \n",
    "'MSSubClass'   \n",
    "'EnclosedPorch'    \n",
    "\n",
    "All others will be classified as Medium \n",
    "\n",
    "This insights results in an change of the potential importance of the features in the columns analysis, stated now in the Correlation column  \n",
    "  \n",
    "Possible siblings(high correlation) within high important variables are:  \n",
    "* GarageArea and **GarageCars**:  \n",
    "    The more cars fit in, the larger the garage has to be and vice versa  \n",
    "\n",
    "* **TotalBsmtSF** and 1stFlrSF:  \n",
    "    The first floor is mostly a similar size to the basement if existing  \n",
    "      \n",
    "* **YearBuilt**, YearRemodAdd and GarageYrBlt:  \n",
    "    Most of the time the garage is build together with the house  \n",
    "    YearRemodAdd is the same as the building year when not remodified since then  \n",
    "    Best of them is YearBuilt \n",
    "  \n",
    "  \n",
    "* **GrLivArea** and TotRmsAbvGrd:  \n",
    "    If the living area is bigger, potentially more rooms are in the house \n",
    "    Decision: GrLivArea  \n",
    "  \n"
   ],
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#removing the siblings from cols\n",
    "cols_clean = list(set(cols) - set([\"GarageArea\", \"1stFlrSF\", \"YearRemodAdd\", \"GarageYrBlt\", \"TotRmsAbvGrd\"]))\n",
    "cols_all_clean = list(set(cols) - set([\"GarageArea\", \"1stFlrSF\", \"YearRemodAdd\", \"GarageYrBlt\", \"TotRmsAbvGrd\"]))\n",
    "print(cols_clean)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### SalePrice - Analysis of the dependent variable  \n",
    "  \n",
    "SalePrice will be the predicted variable. Let's take a look on it"
   ],
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(X_ana.SalePrice.describe())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "looks like no 0 values (very good), positive skewness(not so good), some big ones at the end.  \n",
    "Taking a picture of it including a normal dist to compare:"
   ],
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig, ax_dist = plt.subplots(figsize=(12,9), sharex=True)\n",
    "mean = X_ana.SalePrice.mean()\n",
    "median = X_ana.SalePrice.median()\n",
    "sns.distplot(X_ana.SalePrice, fit=stats.norm, ax = ax_dist)\n",
    "ax_dist.axvline(mean, color=\"red\", linestyle=\"--\")\n",
    "ax_dist.axvline(median, color=\"green\", linestyle = \"-\")\n",
    "plt.legend({\"Normal Dist\":stats.norm, \"Mean\":mean, \"Median\":median})"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Looks like a  \n",
    "* deviation from normal   \n",
    "* including high peak  \n",
    "* with positive skewness  \n",
    "* and outliers on the right (potentially above 700000).   \n",
    "\n",
    "Measurements in numbers: "
   ],
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"Kurtosis: %f\" % X_ana.SalePrice.kurt())\n",
    "print(\"Skewness: %f\" % X_ana.SalePrice.skew())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Maybe we should transform it with a log function and look at it again\n"
   ],
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#apply log\n",
    "X_ana[\"SalePrice\"] = np.log(X_ana[\"SalePrice\"])\n",
    "\n",
    "#inspect again\n",
    "fig, ax_dist = plt.subplots(figsize=(12,9), sharex=True)\n",
    "mean = X_ana.SalePrice.mean()\n",
    "median = X_ana.SalePrice.median()\n",
    "sns.distplot(X_ana.SalePrice, fit=stats.norm, ax = ax_dist)\n",
    "ax_dist.axvline(mean, color=\"red\", linestyle=\"--\")\n",
    "ax_dist.axvline(median, color=\"green\", linestyle = \"-\")\n",
    "plt.legend({\"Normal Dist\":stats.norm, \"Mean\":mean, \"Median\":median})"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Much better now!**"
   ],
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "How can we describe the relation between the Top 15 relatives and SalePrice?\n",
    "\n",
    "Using scatterplots to take a deeper look at it:"
   ],
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for i, col in enumerate([c for c in cols_clean if c != \"SalePrice\"]):\n",
    "    fig, _ = plt.subplots(figsize=(12,9))\n",
    "    sns.scatterplot(x=col, y=\"SalePrice\", data = X_ana)\n",
    "    plt.show()\n",
    "#sns.pairplot(data = X_ana[cols_clean])\n",
    ""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now a detailed analysis for each of the remainind Top columns via visual inspection starting with **OverallQual**:"
   ],
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#OverallQual\n",
    "sns.jointplot(x=\"OverallQual\", y=\"SalePrice\", data = X_ana )"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "OverallQual seems to be exponentially related to Sales price with a wider spread at the upper end of the spectrum  \n",
    "  \n",
    "**Conclusion:** looks like a linear correlation to log(SalePrice)  \n",
    "  \n",
    "Next one will be 'GrLivArea'"
   ],
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#GrLivArea\n",
    "sns.jointplot(x=\"GrLivArea\", y=\"SalePrice\", data = X_ana, height=9, ratio=6 )"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The relation could be a really steep linear with tendency to log relation based on the upper spectrum of X.  \n",
    "We need to further test it with a proper transformation"
   ],
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "GrLivArea_clean = np.exp(X_ana[\"GrLivArea\"])\n",
    "print(GrLivArea_clean.dropna())\n",
    "sns.jointplot(x=\"GrLivArea\", y=\"SalePrice\", data = X_ana, height=9, ratio=6 )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(cols_clean)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Further Analysis"
   ],
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "X = X_raw.copy()\n",
    "X.dropna(subset=[\"SalePrice\"], axis=0, inplace=True)\n",
    "#X.dropna(axis=1, inplace = True)\n",
    "y = X.SalePrice\n",
    "X.drop(\"SalePrice\", axis=1, inplace=True)\n",
    "\n",
    "print(X.head())\n",
    "print(X.info())\n",
    "print(X.describe())\n",
    "\n",
    "col_obj = X.select_dtypes(include=[\"object\", \"bool\"]).columns\n",
    "col_obj_above_10 = list([col for col in col_obj if X[col].nunique() > 15])\n",
    "col_obj_below_10 = list(set(col_obj)-set(col_obj_above_10))\n",
    "col_num = X.select_dtypes(exclude=[\"object\", \"bool\"]).columns.values\n",
    "col_with_nan = [col for col in X.columns if X[col].isna().any()]\n",
    "\n",
    "\n",
    "print(\"Summary:\\n\")\n",
    "print(\"Number of columns: {}\".format(len(X.columns)))\n",
    "print(\"Number of object columns: {}\".format(len(col_obj)))\n",
    "print(\"Number of columns with NaNs: {}\".format(len(col_with_nan)))\n",
    "print(\"Number of object columns with NaNs: {}\".format(len(set(col_with_nan)-set(col_num))))\n",
    "print(\"Number of numeric columns with NaNs: {}\".format(len(set(col_with_nan)-set(col_obj))))\n",
    "\n",
    "y.plot(kind=\"hist\")\n",
    "plt.show()\n",
    "%matplotlib inline\n",
    "y.plot()"
   ],
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The "
   ],
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Pipeline and predictor"
   ],
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "from sklearn.ensemble import RandomForestRegressor\n",
    "from sklearn.linear_model import Lars, ElasticNet\n",
    "from sklearn.svm import SVR\n",
    "from sklearn.ensemble import AdaBoostRegressor\n",
    "from sklearn.neural_network import MLPRegressor\n",
    "from xgboost import XGBRegressor\n",
    "from sklearn.pipeline import Pipeline\n",
    "from sklearn.model_selection import GridSearchCV, train_test_split\n",
    "from sklearn.impute import SimpleImputer\n",
    "from sklearn.compose import ColumnTransformer, TransformedTargetRegressor\n",
    "from sklearn.preprocessing import StandardScaler, OneHotEncoder, OrdinalEncoder\n",
    "from sklearn.decomposition import PCA\n",
    "from sklearn.metrics import mean_absolute_error\n",
    "\n"
   ],
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Building the pipeline"
   ],
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "res= {}\n",
    "np.random.seed(1)"
   ],
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "#reg = ElasticNet()\n",
    "\n",
    "X_train, X_valid, y_train, y_valid = train_test_split( X, y, train_size = 0.8, test_size=0.2, random_state=0)\n",
    "reg = XGBRegressor(evals=([X_valid, y_valid]), eval_metric=\"mae\", cv=True, seed=0 )\n",
    "\n",
    "\n",
    "#numeric_transformer = Pipeline([\"imputer\",SimpleImputer(strategy=\"median\")])\n",
    "\n",
    "\n",
    "\n",
    "#raising value error due to lack in synchronization caused by OneHotEncoder and OrdinalEncoder\n",
    "preprocessing = ColumnTransformer(transformers=[(\"numeric\", SimpleImputer(strategy = \"median\"), col_num) ,\n",
    "                                            (\"obj_below_10\", Pipeline([(\"obj_imputer1\", SimpleImputer(strategy = \"most_frequent\")),\n",
    "                                                                        (\"OHE\"            , OneHotEncoder(handle_unknown='ignore'\n",
    "                                                                                                            ))\n",
    "                                                                        ]), col_obj_below_10) ,\n",
    "                                            (\"Neighborhood_\", Pipeline([(\"obj_imputer2\", SimpleImputer(strategy = \"most_frequent\")),\n",
    "                                                                        (\"label_encoder\"  , OrdinalEncoder(categories=[list(X[\"Neighborhood\"].unique())])\n",
    "                                                                                                            )\n",
    "                                                                        ]), [\"Neighborhood\"]),\n",
    "                                            (\"Exterior_2nd\", Pipeline([(\"obj_imputer2\", SimpleImputer(strategy = \"most_frequent\")),\n",
    "                                                                        (\"label_encoder\"  , OrdinalEncoder(categories=[list(X[\"Exterior2nd\"].unique())]))\n",
    "                                                                        ]), [\"Exterior2nd\"])], remainder='drop')\n",
    "\n",
    "\n",
    "my_pipe = Pipeline([(\"preprocessing\",preprocessing),\n",
    "                     #(\"pca\", PCA(n_components = 35)),\n",
    "                     (\"std_scaler\", StandardScaler()),\n",
    "                     (\"reg\",reg)])\n",
    "\n",
    "\n",
    "param_grid = {  #\"preprocessing__numeric__strategy\":[\"median\", \"mean\"],\n",
    "                #\"pca__n_components\": np.linspace(25, 65, 5, dtype=\"int\"),\n",
    "                \"reg__early_stopping_counts\": [50],\n",
    "                \"reg__n_estimators\": [342],\n",
    "                \"reg__learning_rate\":[0.04002371183530562],\n",
    "                \"reg__max_depth\": [5],\n",
    "                \"reg__min_child_weight\": [1.02],\n",
    "                \"reg__gamma\":[0],\n",
    "                \"reg__subsample\":[0.8264488486588846],\n",
    "                \"reg__colsample_bytree\": [0.551988011711525],\n",
    "                \"reg__alpha\":[0.107],\n",
    "                \n",
    "                }\n",
    "cv = 5\n",
    "\n",
    "search = GridSearchCV(my_pipe, param_grid=param_grid, scoring=\"neg_mean_absolute_error\", verbose=10, n_jobs=-2, cv = cv, return_train_score=True)\n",
    "search.fit(X_train,y_train)\n",
    "print(abs(search.best_score_))\n",
    "search_pred = search.predict(X_valid)\n",
    "search_score = mean_absolute_error(y_valid, search_pred)\n",
    "print(search.best_params_)\n",
    "\n",
    "res[search.best_score_] = search.best_params_\n",
    "\n",
    "test_preds = search.predict(X_test)\n",
    "output = pd.DataFrame({\"Id\": X_test.index,\n",
    "                     \"SalePrice\": test_preds})\n",
    "output.to_csv(\"submission_other.csv\", index=False)"
   ],
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "subsample= 0.8265340078198108\n",
    "subsample=0.8264608951160635\n",
    "subsample_best = 0.8264488486588846\n",
    "colsample_0 = 0.5519746590351778\n",
    "colsample_best= 0.551988011711525\n",
    "n_est = 349\n",
    "n_est = 343\n",
    "print(np.random.normal(0.8265340078198108, 0.001, 20))"
   ],
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "np.random.normal(0.5510204081632653, 0.1,20)"
   ],
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "Choose a relatively high learning rate. Generally a learning rate of 0.1 works but somewhere between 0.05 to 0.3 should work for different problems. Determine the optimum number of trees for this learning rate. XGBoost has a very useful function called as “cv” which performs cross-validation at each boosting iteration and thus returns the optimum number of trees required.\n",
    "Tune tree-specific parameters ( max_depth, min_child_weight, gamma, subsample, colsample_bytree) for decided learning rate and number of trees. Note that we can choose different parameters to define a tree and I’ll take up an example here.\n",
    "Tune regularization parameters (lambda, alpha) for xgboost which can help reduce model complexity and enhance performance.\n",
    "Lower the learning rate and decide the optimal parameters .\n"
   ],
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "from sklearn.compose import Tar\n",
    "\n",
    "#reg = XGBRegressor(early_stopping_counts=5, eval_stop=([X_valid, y_valid]) )\n",
    "reg = ElasticNet()\n",
    "\n",
    "X_train, X_valid, y_train, y_valid = train_test_split( X, y, train_size = 0.8, test_size=0.2, random_state=0)\n",
    "\n",
    "\n",
    "\n",
    "#numeric_transformer = Pipeline([\"imputer\",SimpleImputer(strategy=\"median\")])\n",
    "\n",
    "\n",
    "\n",
    "#raising value error due to lack in synchronization caused by OneHotEncoder and OrdinalEncoder\n",
    "preprocessing = ColumnTransformer(transformers=[(\"numeric\", SimpleImputer(strategy = \"median\"), col_num) ,\n",
    "                                            (\"obj_below_10\", Pipeline([(\"obj_imputer1\", SimpleImputer(strategy = \"most_frequent\")),\n",
    "                                                                        (\"OHE\"            , OneHotEncoder(handle_unknown='ignore'\n",
    "                                                                                                            ))\n",
    "                                                                        ]), col_obj_below_10) ,\n",
    "                                            (\"Neighborhood_\", Pipeline([(\"obj_imputer2\", SimpleImputer(strategy = \"most_frequent\")),\n",
    "                                                                        (\"label_encoder\"  , OrdinalEncoder(categories=[list(X[\"Neighborhood\"].unique())])\n",
    "                                                                                                            )\n",
    "                                                                        ]), [\"Neighborhood\"]),\n",
    "                                            (\"Exterior_2nd\", Pipeline([(\"obj_imputer2\", SimpleImputer(strategy = \"most_frequent\")),\n",
    "                                                                        (\"label_encoder\"  , OrdinalEncoder(categories=[list(X[\"Exterior2nd\"].unique())]))\n",
    "                                                                        ]), [\"Exterior2nd\"])], remainder='drop')\n",
    "\n",
    "\n",
    "my_pipe = Pipeline([(\"preprocessing\",preprocessing),\n",
    "                     (\"pca\", PCA(n_components = 42)),\n",
    "                     (\"reg\",reg)])\n",
    "\n",
    "\n",
    "param_grid = {  #\"preprocessing__numeric__strategy\":[\"median\", \"mean\"],\n",
    "                \"pca__n_components\": np.linspace(25, 65, 10, dtype=\"int\"),\n",
    "                #\"reg__n_estimators\": np.linspace(100, 1000, 20, dtype=\"int\"),\n",
    "                \"reg__alpha\":np.linspace(.01,1.5,50),\n",
    "                \"reg__l1_ratio\": np.linspace(.2, .8, 10)\n",
    "                }\n",
    "cv = 5\n",
    "\n",
    "search = GridSearchCV(my_pipe, param_grid=param_grid, scoring=\"neg_mean_absolute_error\", verbose=5, n_jobs=-2, cv = cv)\n",
    "search.fit(X_train,y_train)\n",
    "print(abs(search.best_score_))\n",
    "search_pred = search.predict(X_valid)\n",
    "search_score = mean_absolute_error(y_valid, search_pred)\n",
    "print(search.best_params_)\n",
    "\n",
    "reg = ElasticNet(alpha=.253265, l1_ratio=0.66666666)\n",
    "my_pipe = Pipeline([(\"preprocessing\",preprocessing),\n",
    "                     (\"pca\", PCA(n_components = 42)),\n",
    "                     (\"reg\",reg)])\n",
    "my_pipe.fit(X_train, y_train)\n",
    "y_pred = my_pipe.predict(X_valid)\n",
    "score = mean_absolute_error(y_valid, y_pred)\n",
    "print(score)"
   ],
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "rfr, lars, elnet, svr, ada, mlpr, xgbr = RandomForestRegressor(), LassoLars(), ElasticNet(), SVR(), AdaBoostRegressor(), MLPRegressor(), XGBRegressor()\n",
    "\n",
    "model_instances = [rfr, lars, elnet, svr, ada, mlpr, xgbr]\n",
    "rfr_list =[\"n_estimators\"]\n",
    "rfr_list.append(np.linspace(10, 200, 10, dtype=\"int\"))\n",
    "lars_list = [\"alpha\"]\n",
    "lars_list.append(np.linspace(.01, .2, 10))\n",
    "elnet = \n",
    "\n",
    "models = {}\n",
    "grids = []\n",
    "print(rfr_list)"
   ],
   "execution_count": null,
   "outputs": []
  }
 ]
}