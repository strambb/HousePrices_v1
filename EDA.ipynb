{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Complete Notebook\n",
    "## Module Loader"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd \n",
    "import numpy as np \n",
    "import matplotlib.pyplot as plt \n",
    "import seaborn as sns\n",
    "from scipy import stats\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "sns.set()\n",
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def sqft_to_m2(sqft):\n",
    "    '''Converts sqft to m2\n",
    "    Input: sqft (square feet value)\n",
    "    Returns: converted m² value'''\n",
    "    m2 = sqft / 10.764\n",
    "    return m2\n",
    "\n",
    "outlier_ids = []\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Analysis (General)\n",
    "### Dataloader"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "path = r\"data\\train.csv\"\n",
    "X_raw = pd.read_csv(path, index_col=\"Id\")\n",
    "X_ana = X_raw.copy()\n",
    "X_test = pd.read_csv(path, index_col =\"Id\")\n",
    "print(X_raw.columns)\n",
    ""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### General analysis\n",
    "\n",
    "**The Problem:**  \n",
    "We want to predict houseprices based on several parameters of different segments.\n",
    "\n",
    "Steps:  \n",
    "A. Read the docs and get information about the dataset, its gathering and the environment of the dataset  \n",
    "B. Analyse column by columns and make information about Type, Segment, Expected Importance  \n",
    "C. Test the assumptions with box / scatter plots  \n",
    "D. Write down consolidated information about importance  \n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "A. Dataset information\n",
    "Unfortunately no additional information were provided within the competition  \n",
    "  \n",
    "B. Analysis of every column\n",
    "After revising every column and its describtion from the data_description.csv-file I have segmented and prioritized the different variabels.  \n",
    "The most important ones form my perspective are:  \n",
    "* LotArea\n",
    "* Utilities\n",
    "* Neighborhood\n",
    "* BldgType\n",
    "* OverallQual\n",
    "* OverallCond\n",
    "* TotalBsmtSF\n",
    "* 1stFlrSF\n",
    "* GrLivArea\n",
    "* FullBath\n",
    "* BedroomAbvGr\n",
    "* TotRmsAbvGrd\n",
    "* GarageArea\n",
    "\n",
    "Next I will try to verify this assumptions with a correlation analysis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#X_corr = X_ana.dropna(axis=1)\n",
    "corrmat = X_ana.corr() #X_corr.corr()\n",
    "fig, ax = plt.subplots(figsize=(12, 9))\n",
    "ax1 = sns.heatmap(corrmat)\n",
    "plt.show()\n",
    "fig2, _ =plt.subplots(figsize=(12,9))\n",
    "ax2 = sns.heatmap(corrmat.iloc[:, -1:].nlargest(100, corrmat.columns[-1]))\n",
    "fig3, _ = plt.subplots(figsize=(25,18))\n",
    "\n",
    "n_all = len(X_ana.columns)\n",
    "n_specific = 16\n",
    "\n",
    "cols = corrmat.nlargest(n_specific, \"SalePrice\")[\"SalePrice\"].index \n",
    "cols_all = corrmat.nlargest(n_all, \"SalePrice\")[\"SalePrice\"].index\n",
    "#found missings in X_ana -> resulting in failure of corrcoef\n",
    "#print(X_ana[cols].isna().any().sum())\n",
    "#remove rows with missing values\n",
    "X_ana_no_nan = X_ana.dropna(subset=cols_all, axis=0)\n",
    "\n",
    "#use cleaned DF to proceed here\n",
    "cm = np.corrcoef(X_ana_no_nan[cols_all].values, rowvar=False)\n",
    "ax3 = sns.heatmap(cm, cbar=True,annot=True, xticklabels=cols_all.values, yticklabels=cols_all.values)\n",
    "\n",
    "print(cols.values[1:16], cols.values[-16: -1])\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The Top 15 most correlated columns according to this correlation analysis are  \n",
    "'OverallQual'  \n",
    "'GrLivArea'  \n",
    "'GarageCars'  \n",
    "'GarageArea'  \n",
    "'TotalBsmtSF'  \n",
    "'1stFlrSF'  \n",
    "'FullBath'  \n",
    "'TotRmsAbvGrd'  \n",
    "'YearBuilt'  \n",
    "'YearRemodAdd'  \n",
    "'GarageYrBlt'  \n",
    "'MasVnrArea'  \n",
    "'Fireplaces'  \n",
    "'BsmtFinSF1'  \n",
    "'LotFrontage'   \n",
    "\n",
    "The 15 least correlated columns according to this correlation analysis are  \n",
    "'BsmtUnfSF'  \n",
    "'BedroomAbvGr'  \n",
    "'ScreenPorch'  \n",
    "'PoolArea'  \n",
    "'MoSold'  \n",
    "'3SsnPorch'   \n",
    "'BsmtFinSF2'  \n",
    "'BsmtHalfBath'  \n",
    "'MiscVal'  \n",
    "'LowQualFinSF'  \n",
    "'YrSold'    \n",
    "'OverallCond'   \n",
    "'MSSubClass'   \n",
    "'EnclosedPorch'    \n",
    "\n",
    "All others will be classified as Medium \n",
    "\n",
    "This insights results in an change of the potential importance of the features in the columns analysis, stated now in the Correlation column  \n",
    "  \n",
    "Possible siblings(high correlation) within high important variables are:  \n",
    "* GarageArea and **GarageCars**:  \n",
    "    The more cars fit in, the larger the garage has to be and vice versa  \n",
    "\n",
    "* **TotalBsmtSF** and 1stFlrSF:  \n",
    "    The first floor is mostly a similar size to the basement if existing  \n",
    "      \n",
    "* **YearBuilt**, YearRemodAdd and GarageYrBlt:  \n",
    "    Most of the time the garage is build together with the house  \n",
    "    YearRemodAdd is the same as the building year when not remodified since then  \n",
    "    Best of them is YearBuilt \n",
    "  \n",
    "  \n",
    "* **GrLivArea** and TotRmsAbvGrd:  \n",
    "    If the living area is bigger, potentially more rooms are in the house \n",
    "    Decision: GrLivArea  \n",
    "  \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#removing the siblings from cols\n",
    "cols_clean = list(set(cols) - set([\"GarageArea\", \"1stFlrSF\", \"YearRemodAdd\", \"GarageYrBlt\", \"TotRmsAbvGrd\"]))\n",
    "cols_all_clean = list(set(cols) - set([\"GarageArea\", \"1stFlrSF\", \"YearRemodAdd\", \"GarageYrBlt\", \"TotRmsAbvGrd\"]))\n",
    "print(cols_clean)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### SalePrice - Analysis of the dependent variable  \n",
    "  \n",
    "SalePrice will be the predicted variable. Let's take a look on it"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(X_ana.SalePrice.describe())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "looks like no 0 values (very good), positive skewness(not so good), some big ones at the end.  \n",
    "Taking a picture of it including a normal dist to compare:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig, ax_dist = plt.subplots(figsize=(12,9), sharex=True)\n",
    "mean = X_ana.SalePrice.mean()\n",
    "median = X_ana.SalePrice.median()\n",
    "sns.distplot(X_ana.SalePrice, fit=stats.norm, ax = ax_dist)\n",
    "ax_dist.axvline(mean, color=\"red\", linestyle=\"--\")\n",
    "ax_dist.axvline(median, color=\"green\", linestyle = \"-\")\n",
    "plt.legend({\"Normal Dist\":stats.norm, \"Mean\":mean, \"Median\":median})"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Looks like a  \n",
    "* deviation from normal   \n",
    "* including high peak  \n",
    "* with positive skewness  \n",
    "* and outliers on the right (potentially above 700000).   \n",
    "\n",
    "Measurements in numbers: "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"Kurtosis: %f\" % X_ana.SalePrice.kurt())\n",
    "print(\"Skewness: %f\" % X_ana.SalePrice.skew())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Maybe we should transform it with a log function and look at it again\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#apply log\n",
    "X_ana[\"SalePrice\"] = np.log1p(X_ana[\"SalePrice\"])\n",
    "\n",
    "#inspect again\n",
    "fig, ax_dist = plt.subplots(figsize=(12,9), sharex=True)\n",
    "mean = X_ana.SalePrice.mean()\n",
    "median = X_ana.SalePrice.median()\n",
    "sns.distplot(X_ana.SalePrice, fit=stats.norm, ax = ax_dist)\n",
    "ax_dist.axvline(mean, color=\"red\", linestyle=\"--\")\n",
    "ax_dist.axvline(median, color=\"green\", linestyle = \"-\")\n",
    "plt.legend({\"Normal Dist\":stats.norm, \"Mean\":mean, \"Median\":median})"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Much better now!**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Univariante analysis of all variables"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#reminder for the list of all variables\n",
    "features = X_ana.copy()\n",
    "all_cols = X_ana.columns.values\n",
    "print(all_cols)\n",
    "\n",
    "#define all numeric types\n",
    "numeric_types = ['int16', 'int32', 'int64', 'float16', 'float32', 'float64']\n",
    "\n",
    "#print overview for all other types\n",
    "for col in all_cols:\n",
    "    print(X_ana[col].describe())\n",
    "    print(\"# of unique values: %i\" %X_ana[col].value_counts().count())\n",
    "    print(\"# of NaNs: %i\" %X_ana[[col]].isna().sum().values)\n",
    "    if X_ana[col].dtype in numeric_types :\n",
    "        sns.distplot(X_ana[col],hist= True, kde=False)\n",
    "        print(\"Collerlation to SalePrice: %f\" %corrmat.loc[\"SalePrice\",col])\n",
    "        \n",
    "    else:\n",
    "        sns.boxplot(x=col, y=\"SalePrice\", data=X_ana, order=X_ana.groupby(col)[\"SalePrice\"].mean().sort_values().index)\n",
    "    plt.show()\n",
    "    print(\"\\n\")    \n",
    "\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#### Finding the LotFrontage correlation\n",
    "\n",
    "correlmat = X_raw.iloc[:, X_raw.columns != \"SalePrice\"].corr()\n",
    "correlmat = correlmat.apply(lambda x: x.abs())\n",
    "colsmax = correlmat[\"LotFrontage\"].nlargest(150).index\n",
    "#colsmin = correlmat[\"LotFrontage\"].nsmallest(10).index\n",
    "#cols = colsmax.append(colsmin)\n",
    "cols = colsmax\n",
    "cm2 = correlmat.loc[cols, cols]\n",
    "sns.heatmap(cm2, xticklabels=cols, yticklabels=cols)\n",
    "#specific_value = correlmat.loc[\"LotFrontage\", \"Neighborhood\"]\n",
    "print(cols)\n",
    "\n",
    "stdevs_LotFrontage = {}\n",
    "for col in X_raw.select_dtypes(\"object\"):\n",
    "    std_value = X_raw.groupby(col)[\"LotFrontage\"].mean().std()\n",
    "    stdevs_LotFrontage[col]= std_value\n",
    "    #print(\"Variable: {} with std of {}\".format(col, std_value))\n",
    "#print(X_raw.groupby([\"LotShape\", \"LotConfig\",\"BldgType\"])[\"LotFrontage\"].median())\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "How can we describe the relation between the Top 15 relatives and SalePrice?\n",
    "\n",
    "Using jointplots to take a deeper look at it:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#for i, col in enumerate([c for c in cols_clean if c != \"SalePrice\"]):\n",
    "#    fig, _ = plt.subplots(figsize=(12,9))\n",
    "#    sns.scatterplot(x=col, y=\"SalePrice\", data = X_ana)\n",
    "#    plt.show()\n",
    "sns.pairplot(data = X_ana[cols_clean[:3]], corner=True, kind=\"reg\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now a detailed analysis for each of the remainind Top columns via visual inspection starting with **OverallQual**:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#OverallQual\n",
    "sns.jointplot(x=\"OverallQual\", y=\"SalePrice\", data = X_ana )"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "OverallQual seems to be exponentially related to Sales price with a wider spread at the upper end of the spectrum  \n",
    "  \n",
    "**Conclusion:** looks like a linear correlation to log(SalePrice)  \n",
    "  \n",
    "Next one will be 'GrLivArea'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#GrLivArea\n",
    "sns.jointplot(x=\"GrLivArea\", y=\"SalePrice\", data = X_ana, height=9, ratio=6 )"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The relation could be a really steep linear with tendency to log relation based on the upper spectrum of X.  \n",
    "Log-Transformation does the trick here.\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_ana[\"GrLivArea\" ] = np.log(X_ana[\"GrLivArea\"])\n",
    "sns.jointplot(x=\"GrLivArea\", y=\"SalePrice\", data = X_ana, height=9, ratio=6 )"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "No it looks quite nice, doesn't it?\n",
    "\n",
    "Next one is GarageCars"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sns.jointplot(x=\"GarageCars\", y=\"SalePrice\", data=X_ana, kind=\"reg\",height=9, ratio=6 )"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This distributition looks very linear to me with a few outliers on the upper end (somekind of pattern...)\n",
    "\n",
    "Next one: TotalBsmtSF"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sns.jointplot(x='TotalBsmtSF', y=\"SalePrice\", data=X_ana, height = 9, ratio= 6)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(sqft_to_m2(6100))\n",
    "TBSF_out = X_ana[X_ana[\"TotalBsmtSF\"]>6000].index.values\n",
    "print(\"ID with mega big basement: %i\" % TBSF_out)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Aright: There must be a house (ID 1299) with a really, realy big basement :D Like 6100 sqft or 567 m² big.  \n",
    "Beside that, the curve looks loggy to me.\n",
    "On the other hand is a larger group of zeros present. This could be an indication for houses without basement.\n",
    "\n",
    "Transformation:  \n",
    "* additional column [HasBasement] with 0 or 1 if value of TotalBsmtSF = 0 or > 0\n",
    "* change of values = 0 to 1\n",
    "* np.log for values > 0\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_ana[\"HasBasement\"] = X_ana['TotalBsmtSF'].apply(lambda x: 1 if x > 0 else 0)\n",
    "X_ana[\"TotalBsmtSF\"] = X_ana['TotalBsmtSF'].apply(lambda x: -999 if x == 0 else x)\n",
    "X_ana[\"TotalBsmtSF\"] = np.log(X_ana[\"TotalBsmtSF\"])\n",
    "sns.jointplot(x='TotalBsmtSF', y=\"SalePrice\", data=X_ana, height = 9, ratio= 6)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Looks way better.\n",
    "\n",
    "Next one: Year Build"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#plot pairplot\n",
    "sns.catplot(kind=\"box\", x=\"YearBuilt\", y=\"SalePrice\", data=X_ana, height=9)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Looks like a slightly exponentail growth over time but this cannot be verified due to lack of information about potential inflation considered in the prices\n",
    "\n",
    "Next one: MasVnrArea"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#scatterplot of MasVnrArea against log(SalePrice)\n",
    "sns.jointplot(x=\"MasVnrArea\", y=\"SalePrice\", data=X_ana, height=9, ratio=6)\n",
    "n_zeros = X_ana[\"MasVnrArea\"].value_counts().sort_index()[0]\n",
    "print(\"Count of 0 in MasVnrArea: %i\" % n_zeros)\n",
    "print(\"Count of rest: %i\" % (len(X_ana)-(n_zeros)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "There are a lot of houses without Masonry. The solution could be a colum has Masonry or not and dropping the original one. ->column simplification\n",
    "On the other hand, the remaining 599 houses with data about masonry are in a medium correlation to Sales Price...  \n",
    "  \n",
    "Transformation:  \n",
    "Do nothing about...\n",
    "\n",
    "Next one:\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(cols_clean)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Further Analysis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "Choose a relatively high learning rate. Generally a learning rate of 0.1 works but somewhere between 0.05 to 0.3 should work for different problems. Determine the optimum number of trees for this learning rate. XGBoost has a very useful function called as “cv” which performs cross-validation at each boosting iteration and thus returns the optimum number of trees required.\n",
    "Tune tree-specific parameters ( max_depth, min_child_weight, gamma, subsample, colsample_bytree) for decided learning rate and number of trees. Note that we can choose different parameters to define a tree and I’ll take up an example here.\n",
    "Tune regularization parameters (lambda, alpha) for xgboost which can help reduce model complexity and enhance performance.\n",
    "Lower the learning rate and decide the optimal parameters .\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "from sklearn.compose import Tar\n",
    "\n",
    "#reg = XGBRegressor(early_stopping_counts=5, eval_stop=([X_valid, y_valid]) )\n",
    "reg = ElasticNet()\n",
    "\n",
    "X_train, X_valid, y_train, y_valid = train_test_split( X, y, train_size = 0.8, test_size=0.2, random_state=0)\n",
    "\n",
    "\n",
    "\n",
    "#numeric_transformer = Pipeline([\"imputer\",SimpleImputer(strategy=\"median\")])\n",
    "\n",
    "\n",
    "\n",
    "#raising value error due to lack in synchronization caused by OneHotEncoder and OrdinalEncoder\n",
    "preprocessing = ColumnTransformer(transformers=[(\"numeric\", SimpleImputer(strategy = \"median\"), col_num) ,\n",
    "                                            (\"obj_below_10\", Pipeline([(\"obj_imputer1\", SimpleImputer(strategy = \"most_frequent\")),\n",
    "                                                                        (\"OHE\"            , OneHotEncoder(handle_unknown='ignore'\n",
    "                                                                                                            ))\n",
    "                                                                        ]), col_obj_below_10) ,\n",
    "                                            (\"Neighborhood_\", Pipeline([(\"obj_imputer2\", SimpleImputer(strategy = \"most_frequent\")),\n",
    "                                                                        (\"label_encoder\"  , OrdinalEncoder(categories=[list(X[\"Neighborhood\"].unique())])\n",
    "                                                                                                            )\n",
    "                                                                        ]), [\"Neighborhood\"]),\n",
    "                                            (\"Exterior_2nd\", Pipeline([(\"obj_imputer2\", SimpleImputer(strategy = \"most_frequent\")),\n",
    "                                                                        (\"label_encoder\"  , OrdinalEncoder(categories=[list(X[\"Exterior2nd\"].unique())]))\n",
    "                                                                        ]), [\"Exterior2nd\"])], remainder='drop')\n",
    "\n",
    "\n",
    "my_pipe = Pipeline([(\"preprocessing\",preprocessing),\n",
    "                     (\"pca\", PCA(n_components = 42)),\n",
    "                     (\"reg\",reg)])\n",
    "\n",
    "\n",
    "param_grid = {  #\"preprocessing__numeric__strategy\":[\"median\", \"mean\"],\n",
    "                \"pca__n_components\": np.linspace(25, 65, 10, dtype=\"int\"),\n",
    "                #\"reg__n_estimators\": np.linspace(100, 1000, 20, dtype=\"int\"),\n",
    "                \"reg__alpha\":np.linspace(.01,1.5,50),\n",
    "                \"reg__l1_ratio\": np.linspace(.2, .8, 10)\n",
    "                }\n",
    "cv = 5\n",
    "\n",
    "search = GridSearchCV(my_pipe, param_grid=param_grid, scoring=\"neg_mean_absolute_error\", verbose=5, n_jobs=-2, cv = cv)\n",
    "search.fit(X_train,y_train)\n",
    "print(abs(search.best_score_))\n",
    "search_pred = search.predict(X_valid)\n",
    "search_score = mean_absolute_error(y_valid, search_pred)\n",
    "print(search.best_params_)\n",
    "\n",
    "reg = ElasticNet(alpha=.253265, l1_ratio=0.66666666)\n",
    "my_pipe = Pipeline([(\"preprocessing\",preprocessing),\n",
    "                     (\"pca\", PCA(n_components = 42)),\n",
    "                     (\"reg\",reg)])\n",
    "my_pipe.fit(X_train, y_train)\n",
    "y_pred = my_pipe.predict(X_valid)\n",
    "score = mean_absolute_error(y_valid, y_pred)\n",
    "print(score)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "rfr, lars, elnet, svr, ada, mlpr, xgbr = RandomForestRegressor(), LassoLars(), ElasticNet(), SVR(), AdaBoostRegressor(), MLPRegressor(), XGBRegressor()\n",
    "\n",
    "model_instances = [rfr, lars, elnet, svr, ada, mlpr, xgbr]\n",
    "rfr_list =[\"n_estimators\"]\n",
    "rfr_list.append(np.linspace(10, 200, 10, dtype=\"int\"))\n",
    "lars_list = [\"alpha\"]\n",
    "lars_list.append(np.linspace(.01, .2, 10))\n",
    "elnet = \n",
    "\n",
    "models = {}\n",
    "grids = []\n",
    "print(rfr_list)"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python",
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "version": "3.7.7-final"
  },
  "orig_nbformat": 2,
  "file_extension": ".py",
  "mimetype": "text/x-python",
  "name": "python",
  "npconvert_exporter": "python",
  "pygments_lexer": "ipython3",
  "version": 3,
  "kernelspec": {
   "name": "python37664bitdsproj10320conda7d2d0b8284c34340aa156a3daf2ddb47",
   "display_name": "Python 3.7.6 64-bit ('DS_proj1_03-20': conda)"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}